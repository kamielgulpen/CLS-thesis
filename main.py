from pyclbr import Class
from networkx.algorithms.centrality import group

from descriptive import Person_links

from itertools import product
import math
import random
import pandas as pd
import networkx as nx
import numpy as np
import multiprocessing as mp
import matplotlib.pyplot as plt
from pathlib import Path
import time


def hash_groups():
    '''
    Making a hash dictionary based on the groups
    Making the hash and rehash dictionaries
    '''

    # Initializing hash dictionaries
    hash_dict = {}
    rehash_dict = {}

    # Read oplniv dataframe
    df = pd.read_csv('Data/tab_n_(with oplniv).csv')

    # Hash every group
    for i in range(df.shape[0]):
        group = df.iloc[i]
                
        age = group['lft']
        etnc = group['etngrp']
        gender = group['geslacht']
        education = group['oplniv']


        hash_dict[f'{age}, {etnc}, {gender}, {education}'] = i
        rehash_dict[i] = f'{age}, {etnc}, {gender}, {education}' 
    
    return hash_dict, rehash_dict


def initialize_nodes(df, hash_dict):
    '''
    Initilizes all the nodes for the network
    '''

    # Initialize list with nodes and nodes per group (example: Man,"[0,20)",1,Autochtoon)
    all_nodes = []
    group_nodes = {}
    
    id = 0

    # Loops through all lines in the tab_n file and get group properties of each line
    for i in range(df.shape[0]):
        group = df.iloc[i]

        
        age = group['lft']
        etnc = group['etngrp']
        gender = group['geslacht']
        education = group['oplniv']
        amount = group['n']
        nodes = []

        # Makes n (the size of the group) nodes of the group
        for _ in range(math.ceil((group['n']))):
 
            node = (id)
            nodes.append(node)
            all_nodes.append(node)
            # all_nodes.append(node)
            id += 1
        # make a dictionary with as key the group properties and value a list of nodes of the group
        group_nodes[hash_dict[f'{age}, {etnc}, {gender}, {education}']] = nodes

        

    return all_nodes, group_nodes


def initialize_edges_links(df_edges,all_nodes, layer ,group_nodes, hash_dict, id_source = None, id_destination = None, source = None, destination = None, barabasi = False, percentage=1, reciprocity=0):     
    '''
    Initializes the links based on the links per group instead of the probability
    '''   
    
    # Initialize all nodes with own link dictionary
    link_dictionary = {}

    for node in all_nodes:
        link_dictionary[node] = set()
      
    

    # Make from all strings integers so we do not have toe loop over pandas dataframe but over numpy array
    initial_list = []

    symetry_dict = {}


    for row in df_edges.iterrows():


        row = row[1]

        dst = hash_dict[f"{row['lft_dst']}, {row['etngrp_dst']}, {row['geslacht_dst']}, {row['oplniv_dst']}"]
        src = hash_dict[f"{row['lft_src']}, {row['etngrp_src']}, {row['geslacht_src']}, {row['oplniv_src']}"]

        initial_list.append(
            (
            dst,
            src,
            int(row['n'])
            )
            )
        
        symetry_dict[(src,dst)] = 0
    initial_list = np.array(initial_list)


    # If layer is huishouden or familie set symetric to True
    if layer == 'huishouden' or layer == 'familie':
        reciprocity = 0.5

    
    # Initializes the source and destinations lists if not multiprocessed
    if source == None and destination == None:
        source = []
        destination = []
        id_source = []
        id_destination = []

    total_edges = sum(df_edges['n'])

    edges_layd = 0
  
    # Loops through all connections (generated by the initialize node function) and makes links
    for row in initial_list:

        # Identifies source and destination group
        src_group = row[0]
        dst_group = row[1]

        connections = row[2]

        # If te data is symetric (familie, household), initiate a dictionary for the destination group
        if reciprocity:
            connections = connections - symetry_dict[(dst_group, src_group)]
     
        # If there are no connections between the source group and the destination group we continue
        if connections == 0:
            continue

        # Initialize dictionary with the node id as key and initial edges, 1, as value
        dst_nodes = group_nodes[dst_group]
        src_nodes = group_nodes[src_group]
        
        # If Barabasi parameter is on take a sample to put in the initial bin (1 percent is standard)
        if barabasi: 
            dst_nodes_bin = random.sample(dst_nodes, k=(math.ceil(len(dst_nodes)*(percentage/100))))
        
        i = 0
        while i < connections:

            if edges_layd % 100000 == 0:
                print(edges_layd/total_edges)
                print(edges_layd)
            
            while True:
                
                # Take random source node and destination node based on the groups
                if len(src_nodes) > 0 and len(dst_nodes) > 0:
                    
                    src_node = random.choices(src_nodes)[0]
                
                    dst_node = random.choices(dst_nodes)[0]
                    
                    # Take random node from the bin
                    if barabasi: dst_node = random.choices(dst_nodes_bin)[0]
                   

                else:
                    break


                # Checks if the source and destination node are not the same and checks if they aren't already linked
                if dst_node != src_node and dst_node not in link_dictionary[src_node]:
                    
                    # If Barabasi append a random node to the bin
                    if barabasi:
                        dst_nodes_bin.append(random.choices(dst_nodes)[0])
               
                        # Add the chosen node to the bin so the chosen node gets a higher weight 
                        if np.random.uniform() < barabasi : dst_nodes_bin.append(dst_node)

                        

                    # Appends both nodes to lists
                    if reciprocity > np.random.uniform() and src_node not in link_dictionary[dst_node]:


                        source.append(dst_group)
                        destination.append(src_group)
                        link_dictionary[dst_node].add(src_node)
                    
                        id_source.append(dst_node)
                        id_destination.append(src_node)
                        
                        symetry_dict[(src_group, dst_group)] += 1

                        if dst_nodes == src_nodes:
                            i += 1
                    source.append(src_group)
                    destination.append(dst_group)
                
                    id_source.append(src_node)
                    id_destination.append(dst_node)

                    link_dictionary[src_node].add(dst_node)

                    break
                
            edges_layd += 1
            i += 1

    print(edges_layd)   
    return source, destination, id_source, id_destination



if __name__ == '__main__':
    # TODO
    # Reciprocity parameter
    # Interlayer correlation

    '''
    Hash function
    '''

    hash_dict, rehash_dict = hash_groups()
    
    '''
    Initialize nodes 
    '''
  
    df_nodes = pd.read_csv('Data/tab_n_(with oplniv).csv')
    all_nodes, group_nodes = initialize_nodes(df_nodes, hash_dict)


    '''
    Initialize edges for multiple layers
    '''

    layers ='huishouden', 'familie','werkschool','buren' 

    barabasi = 0

    percentage = 1

    reciprocity = 1
    
    for layer_number, layer in enumerate(layers):
        
    
        df_edges = pd.read_csv(f'Data/tab_{layer}.csv')
        source, destination, source_id, destination_id = initialize_edges_links(df_edges, all_nodes, layer ,group_nodes, hash_dict, id_source = None, id_destination = None, source = None, destination = None, barabasi = barabasi, percentage = percentage, reciprocity = reciprocity)


        d = {'source_id': list(source_id), 'destination_id':list(destination_id), 'source_group': list(source), 'destination_group': list(destination)}

        df_ = pd.DataFrame(d)
        
        df_.to_csv(f'Data/NW_data/{layer}_nw_b={barabasi}_{percentage}_percentage.csv')


    

    


    