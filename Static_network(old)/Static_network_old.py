from pyclbr import Class
from networkx.algorithms.centrality import group
from network import Network
from descriptive import Person_links
from group import Group
from node import Node
from itertools import product
import math
import random
import pandas as pd
import networkx as nx
import numpy as np
import multiprocessing as mp
import matplotlib.pyplot as plt
from pathlib import Path
import time

def hash_groups():
    hash_dict = {}
    rehash_dict = {}

    df = pd.read_csv('Data/tab_n_(with oplniv).csv')

    for i in range(df.shape[0]):
        group = df.iloc[i]
                
        age = group['lft']
        etnc = group['etngrp']
        gender = group['geslacht']
        education = group['oplniv']


        hash_dict[f'{age}, {etnc}, {gender}, {education}'] = i
        rehash_dict[i] = f'{age}, {etnc}, {gender}, {education}' 
    
    return hash_dict, rehash_dict


def initialize_nodes(df, division, hash_dict):
    '''
    Initilizes all the nodes for the network
    '''

    # Initialize list with nodes and nodes per group (example: Man,"[0,20)",1,Autochtoon)
    all_nodes = []
    group_nodes = {}
    
    id = 0

    # Loops through all lines in the tab_n file and get group properties of each line
    for i in range(df.shape[0]):
        group = df.iloc[i]

        
        age = group['lft']
        etnc = group['etngrp']
        gender = group['geslacht']
        education = group['oplniv']
        amount = group['n']
        nodes = []

        # Makes n (the size of the group) nodes of the group
        for _ in range(math.ceil((group['n'])/division)):
 
            node = (id)
            nodes.append(node)
            all_nodes.append(node)
            # all_nodes.append(node)
            id += 1
        # make a dictionary with as key the group properties and value a list of nodes of the group
        group_nodes[hash_dict[f'{age}, {etnc}, {gender}, {education}']] = nodes

        

    return all_nodes, group_nodes

def initialize_edges_probability(df_edges, all_nodes, group_nodes, source = None, destination = None):
    '''
    Initializes the edges of a chosen network (household, family etc.), based on probability
    '''

    # Initializes the source and destinations lists
    source = []
    destination = []

    
    # Loops through all nodes (generated by the initialize node function) and makes links
    for count, node in enumerate(all_nodes):
        
        # Get persons based on the nodes poperties
        person = Person_links(df_edges,node.gender, node.age, node.education, node.etnicity)

        # Get the total amount of links the person group has
        group_links = sum(person.links['n'])

        # Divide the groups link by the amout of nodes so every node gets the same amount of links
        links = group_links/node.amount

        # Update
        if count % 100 == 0:
            print(count/len(all_nodes))

        # Make link based on probability
        prob = person.links['fn']/sum(person.links['fn'])
        
        # Get a list with all the destination groups the current group has
        groups = [f'{age}, {etnc}, {gender}, {education}' for age, etnc, gender, education in zip(person.links['lft_dst'],
        person.links['etngrp_dst'], person.links['geslacht_dst'], person.links['oplniv_dst'])]


        
        # Checks if probability is higher than 0
        if sum(prob) == 0:
            continue

        
        # Get a list (of length links) of destination nodes based on the probability of the link
        destination_nodes = np.random.choice(groups, round(links), p=prob)

        # Loop through the destination nodes and make connection
        for person in destination_nodes:
            
            # Take node id as source
            source.append(node.id)

            # Choose random node from the destination group and make destination node
            try : 
                node2 = np.random.choice(group_nodes[person])
            except:
                print(group_nodes[person])
                print(person)

            destination.append(node2.id)
            
        #     # G.add_edge(node.id, node2.id)
    return source, destination

    


def initialize_edges_links(df_edges,all_nodes, layer ,group_nodes, hash_dict, division, id_source = None, id_destination = None, source = None, destination = None, barabasi = False):     
    '''
    Initializes the links based on the links per group instead of the probability
    '''   
    
    # Initialize all nodes with own link dictionary
    link_dictionary = {}

    for node in all_nodes:
        link_dictionary[node] = set()
      
    

    # Make from all strings integers so we do not have toe loop over pandas dataframe but over numpy array
    initial_list = []
    for row in df_edges.iterrows():

        row = row[1]

        dst = hash_dict[f"{row['lft_dst']}, {row['etngrp_dst']}, {row['geslacht_dst']}, {row['oplniv_dst']}"]
        src = hash_dict[f"{row['lft_src']}, {row['etngrp_src']}, {row['geslacht_src']}, {row['oplniv_src']}"]

        initial_list.append(
            (
            dst,
            src,
            int(row['n'])
            )
            )
        
            
        

    initial_list = np.array(initial_list)

    symetric = False

    if layer == 'huishouden' or layer == 'familie':

        symetric = True
        symetry_dict = {}
        src_groups = []
        dst_groups = []

        for row in df_edges.iterrows():

            row = row[1]
            
            dst_group = hash_dict[f"{row['lft_dst']}, {row['etngrp_dst']}, {row['geslacht_dst']}, {row['oplniv_dst']}"]
           
            src_group = hash_dict[f"{row['lft_src']}, {row['etngrp_src']}, {row['geslacht_src']}, {row['oplniv_src']}"]

            src_groups.append(src_group)
            dst_groups.append(dst_group)



        for combination in zip(src_groups, dst_groups):
            symetry_dict[(combination)] = 0


    
    # Initializes the source and destinations lists if not multiprocessed
    if source == None and destination == None:
        source = []
        destination = []
        id_source = []
        id_destination = []

    total_edges = sum(df_edges['n'])

    edges_layd = 0
  
    # Loops through all connections (generated by the initialize node function) and makes links
    for row in initial_list:

        if symetric:
            links2 = []
        links = []

        # Identifies source and destination group
        src_group = row[0]
        dst_group = row[1]

        connections = row[2]

        # If te data is symetric (familie, household), initiate a dictionary for the destination group
        if symetric:
            connections = connections - symetry_dict[(dst_group, src_group)]
     
        # If there are no connections between the source group and the destination group we continue
        if connections == 0:
            continue



        # Initialize dictionary with the node id as key and initial edges, 1, as value
        dst_nodes = group_nodes[dst_group]
        src_nodes = group_nodes[src_group]
        dst_nodes_bin = [random.choices(dst_nodes)[0]]
        dst_nodes_bin = random.sample(dst_nodes, k=(math.ceil(len(dst_nodes)*1)))
        
        # print(len(dst_nodes), len(src_nodes))
        # if barabasi:
        #    
              
        #     source.extend([src_group] * len(src_nodes))
        #     destination.extend([dst_group] * len(dst_nodes))
        
        #     id_source.extend()
        #     id_destination.extend(dst_nodes)

        #     connections -= len(dst_nodes)



        
        for _ in range(connections):
            if edges_layd % 100000 == 0:
                print(edges_layd/total_edges)
                print(edges_layd)
            
            while True:
                
                # Take random source node and destination node based on the groups
                if len(src_nodes) > 0 and len(dst_nodes) > 0:
                    
                    src_node = random.choices(src_nodes)[0]
                
                    dst_node = random.choices(dst_nodes)[0]
                    
                    if barabasi: dst_node = random.choices(dst_nodes_bin)[0]
                   

                else:
                    break


                # Checks if the source and destination node are not the same and checks if they aren't already linked
                if dst_node != src_node and dst_node not in link_dictionary[src_node]:
                    
                    # Not the way!! Begin with 1 node that add the node then add nodes to the bin on the go einlijk in volgorde appenden, misschien eerst random door elkaar husselen?
                    if barabasi:
                        dst_nodes_bin.append(random.choices(dst_nodes)[0])
               
                        
                        if np.random.uniform() < barabasi : dst_nodes_bin.append(dst_node)

                        

                    # Appends both nodes to lists
                    if symetric and src_node not in link_dictionary[dst_node]:


                        source.append(dst_group)
                        destination.append(src_group)
                        link_dictionary[dst_node].add(src_node)
                    
                        id_source.append(dst_node)
                        id_destination.append(src_node)
                        
                        symetry_dict[(src_group, dst_group)] += 1
                    
                    source.append(src_group)
                    destination.append(dst_group)
                
                    id_source.append(src_node)
                    id_destination.append(dst_node)

                    link_dictionary[src_node].add(dst_node)

                    break
                
            edges_layd += 1

    print(edges_layd)   
    return source, destination, id_source, id_destination



if __name__ == '__main__':
    # Todo
    # Reciprocity parameter
    # Interlayer correlation


    '''
    Hash function
    '''

    hash_dict, rehash_dict = hash_groups()
    
    '''
    Initialize nodes 
    '''
    begin = time.time()
    division = 1
    df_nodes = pd.read_csv('Data/tab_n_(with oplniv).csv')
    all_nodes, group_nodes = initialize_nodes(df_nodes, division, hash_dict)
    end = time.time()

    print(end-begin)


    '''
    Initialize edges for multiple layers
    '''

    layers ='werkschool', 'x', 'familie'
    barabasi = 1
    
    for layer_number, layer in enumerate(layers):
        
    
        df_edges = pd.read_csv(f'Data/tab_{layer}.csv')
        source, destination, source_id, destination_id = initialize_edges_links(df_edges, all_nodes, layer ,group_nodes, hash_dict, division, id_source = None, id_destination = None, source = None, destination = None, barabasi = barabasi)

        

        # df_edges = pd.read_csv(f'Data/tab_{layer}.csv')
        # df_edges = np.array_split(df_edges, 6)
    
        # manager = mp.Manager()
        # source = manager.list()
        # destination = manager.list()
        # source_id = manager.list()
        # destination_id = manager.list()
        # all_nodes = np.array_split(all_nodes,6)

        
        # processes = []    
        # for i in range(6):
        #     process = mp.Process(target=initialize_edges_links, args=(df_edges[i], all_nodes[i], layer, group_nodes,division,source_id ,destination_id,source, destination))
        #     processes.append(process)

        # for process in processes:
        #     process.start()

        # for process in processes:
        #     process.join()

        d = {'source_id': list(source_id), 'destination_id':list(destination_id), 'source_group': list(source), 'destination_group': list(destination)}

        df_ = pd.DataFrame(d)

        # np.savetxt(r'Data/NW_data/{layer}_nw_b={barabasi}_small.txt', df_.values, fmt='%d')
        
        df_.to_csv(f'Data/NW_data/{layer}_nw_b={barabasi}_try_100_percentage.csv')


    

    


    